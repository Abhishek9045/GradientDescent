# GradientDescent
In this lab, we'll implement the basic functions of the Gradient Descent algorithm to find the boundary in a small dataset.First, we'll start with some functions that will help us plot and visualize the data.

Implementing the basic functions

Here is your turn to shine. Implement the following formulas, as explained in the text.

Sigmoid activation function
ğœ(ğ‘¥)=11+ğ‘’âˆ’ğ‘¥
 
Output (prediction) formula
ğ‘¦Ì‚ =ğœ(ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+ğ‘)
 
Error function
ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ(ğ‘¦,ğ‘¦Ì‚ )=âˆ’ğ‘¦log(ğ‘¦Ì‚ )âˆ’(1âˆ’ğ‘¦)log(1âˆ’ğ‘¦Ì‚ )
 
The function that updates the weights
ğ‘¤ğ‘–âŸ¶ğ‘¤ğ‘–+ğ›¼(ğ‘¦âˆ’ğ‘¦Ì‚ )ğ‘¥ğ‘–
 
ğ‘âŸ¶ğ‘+ğ›¼(ğ‘¦âˆ’ğ‘¦Ì‚ )

This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm.

Time to train the algorithm!

When we run the function, we'll obtain the following:

10 updates with the current training loss and accuracy
A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.
A plot of the error function. Notice how it decreases as we go through more epochs.

